AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: An AWS SAM template for the Kafka producer and consumer Lambdas with DynamoDB tables.

Parameters:
  EC2KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instance.
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: Must be the name of an existing EC2 KeyPair.

  EC2AMI:
    Description: Amazon Machine Image (AMI) ID for the EC2 instance.
    Type: String
    Default: ami-06db4d78cb1d3bbf9 
    ConstraintDescription: Must be a valid AMI ID.

  ETHERSCANAPIKEY:
    Description: Key for etherscan api
    Type: String
    ConstraintDescription: Without a key, the producer cannot reach to the API
  AIRFLOWFIRSTNAME:
    Description: First name of Airflow admin user
    Type: String
    ConstraintDescription: Please, offer a first name for the admin user of airflow
  AIRFLOWLASTNAME:
    Description: Last name of Airflow admin suer
    Type: String
    ConstraintDescription: Please, offer a last name for the admin user of airflow
  AIRFLOWPASS:
    Description: password for airflow admin user
    Type: String
    ConstraintDescription: Please, offer a password for the admin user of airflow
  AIRFLOWEMAIL:
    Description: email for airflow admin user
    Type: String
    ConstraintDescription: Please, offer an email for the admin user of airflow


Resources:
  BlockConfigTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: BlockConfig
      AttributeDefinitions:
        - AttributeName: config_type
          AttributeType: S
      KeySchema:
        - AttributeName: config_type
          KeyType: HASH
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5

  GasCostPerHourTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: GasCostPerHour
      AttributeDefinitions:
        - AttributeName: DATE
          AttributeType: S
        - AttributeName: HOUR
          AttributeType: S
      KeySchema:
        - AttributeName: DATE
          KeyType: HASH
        - AttributeName: HOUR
          KeyType: RANGE
      ProvisionedThroughput:
        ReadCapacityUnits: 10
        WriteCapacityUnits: 5

  # EC2 Resources
  EC2Instance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.small
      ImageId: !Ref EC2AMI
      KeyName: !Ref EC2KeyName
      SecurityGroups:
        - !Ref EC2SecurityGroup
      Tags:
        - Key: 'Name'
          Value: 'KafkaAirflowServer'
      UserData:
        Fn::Base64:
          !Sub |
            #!/bin/bash
            # Update the system
            sudo apt-get update
            sudo apt install tmux git
            # Install JDK for Kafka
            sudo apt-get install -y default-jdk
            git clone https://github.com/AlejandroFNadal/market_analysis.git
            git checkout dev
            # Download Kafka
            wget "https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz"
            tar -xvzf kafka_2.13-3.6.0.tgz
            cd kafka_2.13-3.6.0
            echo '#!/bin/bash' > /usr/local/bin/configure-kafka.sh
            echo 'PRIVATE_IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)' >> /usr/local/bin/configure-kafka.sh
            echo 'PUBLIC_IP=$(curl http://169.254.169.254/latest/meta-data/public-ipv4)' >> /usr/local/bin/configure-kafka.sh
            echo '# Backup the original config file' >> /usr/local/bin/configure-kafka.sh
            echo 'cp /kafka_2.13-3.6.0/config/server.properties  /kafka_2.13-3.6.0/config/server.properties.backup' >> /usr/local/bin/configure-kafka.sh
            echo '# Update the properties file' >> /usr/local/bin/configure-kafka.sh

            # Uncomment the listeners and advertised.listeners if they are commented
            echo 'sed -i "s/^#listeners=/listeners=/" /kafka_2.13-3.6.0/config/server.properties' >> /usr/local/bin/configure-kafka.sh
            echo 'sed -i "s/^#advertised.listeners=/advertised.listeners=/" /kafka_2.13-3.6.0/config/server.properties' >> /usr/local/bin/configure-kafka.sh
            # replacing the variables.
            echo 'sed -i "s/^listeners=PLAINTEXT:\/\/.*/listeners=PLAINTEXT:\/\/$PRIVATE_IP:9092/"  /kafka_2.13-3.6.0/config/server.properties' >> /usr/local/bin/configure-kafka.sh
            echo 'sed -i "s/^advertised.listeners=PLAINTEXT:\/\/.*/advertised.listeners=PLAINTEXT:\/\/$PUBLIC_IP:9092/" /kafka_2.13-3.6.0/config/server.properties' >> /usr/local/bin/configure-kafka.sh 
            echo 'nohup /kafka_2.13-3.6.0/bin/zookeeper-server-start.sh /kafka_2.13-3.6.0/config/zookeeper.properties &' >> /usr/local/bin/configure-kafka.sh
            echo 'sleep 5' >> /usr/local/bin/configure-kafka.sh
            echo 'nohup /kafka_2.13-3.6.0/bin/kafka-server-start.sh /kafka_2.13-3.6.0/config/server.properties &' >> /usr/local/bin/configure-kafka.sh

            
            # allow execution
            chmod +x /usr/local/bin/configure-kafka.sh
            echo '[Unit]' > /etc/systemd/system/kafka-configure.service
            echo 'Description=Configure Kafka Server Properties' >> /etc/systemd/system/kafka-configure.service
            echo 'After=network.target' >> /etc/systemd/system/kafka-configure.service
            echo ''  >> /etc/systemd/system/kafka-configure.service
            echo '[Service]' >> /etc/systemd/system/kafka-configure.service
            echo 'Type=oneshot' >> /etc/systemd/system/kafka-configure.service
            echo 'ExecStart=/usr/local/bin/configure-kafka.sh' >> /etc/systemd/system/kafka-configure.service
            echo 'RemainAfterExit=yes' >> /etc/systemd/system/kafka-configure.service
            echo '' >> /etc/systemd/system/kafka-configure.service
            echo '[Install]' >> /etc/systemd/system/kafka-configure.service
            echo 'WantedBy=multi-user.target' >> /etc/systemd/system/kafka-configure.service
            systemctl enable kafka-configure.service
            systemctl start kafka-configure.service

            # Install Airflow
            sudo apt-get install -y python3-pip 
            pip install "apache-airflow[celery]==2.7.2" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.7.2/constraints-3.8.txt" --break-system-packages
            # Initialize the Airflow DB (SQLite by default)
            airflow db init
            airflow migrate
            # Create user 
            airflow users create --username my_admin --role Admin --firstname ${AIRFLOWFIRSTNAME} --lastname ${AIRFLOWLASTNAME} --email ${AIRFLOWEMAIL} --password ${AIRFLOWPASS} 
            # Create another daemon to manage airflow
            touch /usr/local/bin/configure-airflow.sh
            echo "#!/bin/bash" >/usr/local/bin/configure-airflow.sh
            echo "export AIRFLOW_ENV_PLACE=aws" >> /usr/local/bin/configure-airflow.sh
            echo "export S3_BUCKET_NAME=${!Ref DataBucket}" >> /usr/local/bin/configure-airflow.sh
            echo "nohup airflow scheduler &" >> /usr/local/bin/configure-airflow.sh
            echo "nohup airflow webserver &" >> /usr/local/bin/configure-airflow.sh
            chmod +x /usr/local/bin/configure-airflow.sh 
            
            echo '[Unit]' > /etc/systemd/system/airflow-configure.service
            echo 'Description=Configure and launch Airflow Server' >> /etc/systemd/system/airflow-configure.service
            echo 'After=network.target' >> /etc/systemd/system/airflow-configure.service
            echo ''  >> /etc/systemd/system/airflow-configure.service
            echo '[Service]' >> /etc/systemd/system/airflow-configure.service
            echo 'Type=oneshot' >> /etc/systemd/system/airflow-configure.service
            echo 'ExecStart=/usr/local/bin/configure-airflow.sh' >> /etc/systemd/system/airflow-configure.service
            echo 'RemainAfterExit=yes' >> /etc/systemd/system/airflow-configure.service
            echo '' >> /etc/systemd/system/airflow-configure.service
            echo '[Install]' >> /etc/systemd/system/airflow-configure.service
            echo 'WantedBy=multi-user.target' >> /etc/systemd/system/airflow-configure.service

            systemctl enable airflow-configure.service
            systemctl start airflow-configure.service
            

  
  EC2SecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Enable SSH access
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: '22'
          ToPort: '22'
          CidrIp: 0.0.0.0/0  # Restrict this to your IP for better security
        - IpProtocol: 'tcp'
          FromPort: 9092
          ToPort: 9092
          CidrIp: '0.0.0.0/0'  # Kafka port
        - IpProtocol: 'tcp'
          FromPort: 8080
          ToPort: 8080
          CidrIp: '0.0.0.0/0'  # Airflow port
  # S3 Bucket
  DataBucket:
    Type: 'AWS::S3::Bucket'

  # Lambda Execution Role
  LambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: LambdaAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${DataBucket}/*
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:Query
                Resource:
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/BlockConfig
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/GasCostPerHour
              - Effect: Allow
                Action:
                  - ec2:DescribeInstances
                Resource:
                  "*"

  ProducerFunction:
    Type: AWS::Serverless::Function
    Properties:
      Handler: producer.handler
      Runtime: python3.10
      CodeUri: producer/
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 30
      Events:
        ProducerTimer:
          Type: Schedule
          Properties:
            Schedule: rate(2 minutes)
      Environment:
        Variables:
          BLOCK_CONFIG_TABLE: BlockConfig
          GAS_COST_TABLE: GasCostPerHour
          KAFKA_SERVER_AWS: True
          ETHERSCAN_API_KEY: !Ref ETHERSCANAPIKEY
          DYNAMODB_SERVER_AWS: True

  ConsumerFunction:
    Type: AWS::Serverless::Function
    Properties:
      Handler: consumer.handler
      Runtime: python3.10
      CodeUri: consumer/
      Timeout: 30
      Role: !GetAtt LambdaExecutionRole.Arn
      Events:
        ConsumerTimer:
          Type: Schedule
          Properties:
            Schedule: rate(2 minutes)
      Environment:
        Variables:
          BLOCK_CONFIG_TABLE: BlockConfig
          GAS_COST_TABLE: GasCostPerHour
          KAFKA_SERVER_AWS: True
          S3_BUCKET_NAME: !Ref DataBucket 
          SAVE_TO_S3: True              
          DYNAMODB_SERVER_AWS: True
            #
  # EC2 Instance Role
  EC2InstanceProfile:
    Type: 'AWS::IAM::InstanceProfile'
    Properties:
      Roles:
        - !Ref EC2InstanceRole

  EC2InstanceRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: EC2Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:Query
                Resource:
                  - !Sub arn:aws:s3:::${DataBucket}/*
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/BlockConfig
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/GasCostPerHour

Outputs:
  ProducerFunctionArn:
    Description: Producer Lambda Function ARN
    Value: !GetAtt ProducerFunction.Arn

  ConsumerFunctionArn:
    Description: Consumer Lambda Function ARN
    Value: !GetAtt ConsumerFunction.Arn

  BlockConfigTableArn:
    Description: BlockConfig DynamoDB Table ARN
    Value: !GetAtt BlockConfigTable.Arn

  GasCostPerHourTableArn:
    Description: GasCostPerHour DynamoDB Table ARN
    Value: !GetAtt GasCostPerHourTable.Arn

 
