AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: An AWS SAM template for the Kafka producer and consumer Lambdas with DynamoDB tables.

Parameters:
  EC2KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to the instance.
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: Must be the name of an existing EC2 KeyPair.

  EC2AMI:
    Description: Amazon Machine Image (AMI) ID for the EC2 instance.
    Type: String
    Default: ami-06db4d78cb1d3bbf9 
    ConstraintDescription: Must be a valid AMI ID.

  ETHERSCANAPIKEY:
    Description: Key for etherscan api
    Type: String
    ConstraintDescription: Without a key, the producer cannot reach to the API
  AIRFLOWFIRSTNAME:
    Description: First name of Airflow admin user
    Type: String
    ConstraintDescription: Please, offer a first name for the admin user of airflow
  AIRFLOWLASTNAME:
    Description: Last name of Airflow admin suer
    Type: String
    ConstraintDescription: Please, offer a last name for the admin user of airflow
  AIRFLOWPASS:
    Description: password for airflow admin user
    Type: String
    ConstraintDescription: Please, offer a password for the admin user of airflow
  AIRFLOWEMAIL:
    Description: email for airflow admin user
    Type: String
    ConstraintDescription: Please, offer an email for the admin user of airflow

Resources:
  # EC2 Resources
  EC2KafkaInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.small
      ImageId: !Ref EC2AMI
      KeyName: !Ref EC2KeyName
      SecurityGroups:
        - !Ref EC2KafkaSecurityGroup
      IamInstanceProfile: !Ref EC2InstanceProfile
      Tags:
        - Key: 'Name'
          Value: 'KafkaServer'
      UserData:
        Fn::Base64:
          !Sub |
            #!/bin/bash
            # Update the system
            sudo apt-get update
            sudo apt install tmux git python3.11-venv -y
            # Install JDK for Kafka
            sudo apt-get install -y default-jdk
            git clone https://github.com/AlejandroFNadal/market_analysis.git
            git checkout dev
            # Download Kafka
            wget "https://downloads.apache.org/kafka/3.6.1/kafka-3.6.1-src.tgz"
            tar -xvzf kafka_2.13-3.6.1.tgz
            cd kafka_2.13-3.6.1
            echo '#!/bin/bash' > /usr/local/bin/configure-kafka.sh
            echo 'PRIVATE_IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)' >> /usr/local/bin/configure-kafka.sh
            echo 'PUBLIC_IP=$(curl http://169.254.169.254/latest/meta-data/public-ipv4)' >> /usr/local/bin/configure-kafka.sh
            echo '# Backup the original config file' >> /usr/local/bin/configure-kafka.sh
            echo 'cp /kafka_2.13-3.6.1/config/server.properties  /kafka_2.13-3.6.1/config/server.properties.backup' >> /usr/local/bin/configure-kafka.sh
            echo '# Update the properties file' >> /usr/local/bin/configure-kafka.sh

            # Uncomment the listeners and advertised.listeners if they are commented
            echo 'sed -i "s/^#listeners=/listeners=/" /kafka_2.13-3.6.1/config/server.properties' >> /usr/local/bin/configure-kafka.sh
            echo 'sed -i "s/^#advertised.listeners=/advertised.listeners=/" /kafka_2.13-3.6.1/config/server.properties' >> /usr/local/bin/configure-kafka.sh
            # replacing the variables.
            echo 'sed -i "s/^listeners=PLAINTEXT:\/\/.*/listeners=PLAINTEXT:\/\/$PRIVATE_IP:9092/"  /kafka_2.13-3.6.1/config/server.properties' >> /usr/local/bin/configure-kafka.sh
            echo 'sed -i "s/^advertised.listeners=PLAINTEXT:\/\/.*/advertised.listeners=PLAINTEXT:\/\/$PUBLIC_IP:9092/" /kafka_2.13-3.6.1/config/server.properties' >> /usr/local/bin/configure-kafka.sh 
            echo 'nohup /kafka_2.13-3.6.0/bin/zookeeper-server-start.sh /kafka_2.13-3.6.1/config/zookeeper.properties &' >> /usr/local/bin/configure-kafka.sh
            echo 'sleep 5' >> /usr/local/bin/configure-kafka.sh
            echo 'nohup /kafka_2.13-3.6.0/bin/kafka-server-start.sh /kafka_2.13-3.6.1/config/server.properties &' >> /usr/local/bin/configure-kafka.sh

            
            # allow execution
            chmod +x /usr/local/bin/configure-kafka.sh
            echo '[Unit]' > /etc/systemd/system/kafka-configure.service
            echo 'Description=Configure Kafka Server Properties' >> /etc/systemd/system/kafka-configure.service
            echo 'After=network.target' >> /etc/systemd/system/kafka-configure.service
            echo ''  >> /etc/systemd/system/kafka-configure.service
            echo '[Service]' >> /etc/systemd/system/kafka-configure.service
            echo 'Type=oneshot' >> /etc/systemd/system/kafka-configure.service
            echo 'ExecStart=/usr/local/bin/configure-kafka.sh' >> /etc/systemd/system/kafka-configure.service
            echo 'RemainAfterExit=yes' >> /etc/systemd/system/kafka-configure.service
            echo '' >> /etc/systemd/system/kafka-configure.service
            echo '[Install]' >> /etc/systemd/system/kafka-configure.service
            echo 'WantedBy=multi-user.target' >> /etc/systemd/system/kafka-configure.service
            systemctl enable kafka-configure.service
            systemctl start kafka-configure.service

   
  EC2KafkaSecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Enable SSH access
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: '22'
          ToPort: '22'
          CidrIp: 0.0.0.0/0  # Restrict this to your IP for better security
        - IpProtocol: 'tcp'
          FromPort: 9092
          ToPort: 9092
          CidrIp: '0.0.0.0/0'  # Kafka port


  EC2AirflowSecGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Allows for Airflow communication, as well as for Kafka consumers to run.
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: '22'
          ToPort: '22'
          CidrIp: 0.0.0.0/0  # Restrict this to your IP for better security
        - IpProtocol: 'tcp'
          FromPort: 8080
          ToPort: 8080
          CidrIp: '0.0.0.0/0'

  EC2AirflowInstance:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: t2.small
      ImageId: !Ref EC2AMI
      KeyName: !Ref EC2KeyName
      SecurityGroups:
        - !Ref EC2AirflowSecGroup
      IamInstanceProfile: !Ref EC2InstanceProfile
      Tags:
        - Key: 'Name'
          Value: 'AirflowServer'
      UserData:
        Fn::Base64:
          Fn::Sub:
          - |
            #!/bin/bash
            sudo apt-get update
            sudo apt install tmux git python3.11-venv -y
            # Install Airflow
            sudo apt-get install -y python3-pip
            pip install "apache-airflow[celery]==2.7.2" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.7.2/constraints-3.8.txt" --break-system-packages
            pip install boto3 --break-system-packages
            # Initialize the Airflow DB (SQLite by default)
            airflow db init
            airflow migrate
            # Create user 
            airflow users create --username my_admin --role Admin --firstname ${AIRFLOWFIRSTNAME} --lastname ${AIRFLOWLASTNAME} --email ${AIRFLOWEMAIL} --password ${AIRFLOWPASS} 
            # Create another daemon to manage airflow
            touch /usr/local/bin/configure-airflow.sh
            echo "#!/bin/bash" >/usr/local/bin/configure-airflow.sh
            echo "export AIRFLOW_ENV_PLACE=aws" >> /usr/local/bin/configure-airflow.sh
            echo "export S3_BUCKET_NAME=${EthAnalysisDataBucket}" >> /usr/local/bin/configure-airflow.sh
            echo 'sed -i "s/load_examples = True=/load_examples = False/" /root/airflow/airflow.cfg' >> /usr/local/bin/configure-kafka.sh
            echo "nohup airflow scheduler &" >> /usr/local/bin/configure-airflow.sh
            echo "nohup airflow webserver &" >> /usr/local/bin/configure-airflow.sh
            chmod +x /usr/local/bin/configure-airflow.sh 
            
            echo '[Unit]' > /etc/systemd/system/airflow-configure.service
            echo 'Description=Configure and launch Airflow Server' >> /etc/systemd/system/airflow-configure.service
            echo 'After=network.target' >> /etc/systemd/system/airflow-configure.service
            echo ''  >> /etc/systemd/system/airflow-configure.service
            echo '[Service]' >> /etc/systemd/system/airflow-configure.service
            echo 'Type=oneshot' >> /etc/systemd/system/airflow-configure.service
            echo 'ExecStart=/usr/local/bin/configure-airflow.sh' >> /etc/systemd/system/airflow-configure.service
            echo 'RemainAfterExit=yes' >> /etc/systemd/system/airflow-configure.service
            echo '' >> /etc/systemd/system/airflow-configure.service
            echo '[Install]' >> /etc/systemd/system/airflow-configure.service
            echo 'WantedBy=multi-user.target' >> /etc/systemd/system/airflow-configure.service

            systemctl enable airflow-configure.service
            systemctl start airflow-configure.service

            # create dag folder
            mkdir /root/airflow/dags

            # prepare consumer task
            git clone https://github.com/AlejandroFNadal/market_analysis  
            cd market_analysis
            git checkout dev
            cd back
            cd consumer
            python3 -m venv env
            source env/bin/activate
            pip3 install -r requirements.txt
            echo 'KAFKA_SERVER_AWS=true' > .env
            echo 'SAVE_TO_S3=true' >> .env
            echo 'BLOCK_CONFIG_TABLE=BlockConfig' >> .env
            echo 'DYNAMODB_SERVER_AWS=true' >> .env
            echo 'GAS_COST_TABLE=GasCostPerHour' >> .env
            echo "S3_BUCKET_NAME=${EthAnalysisDataBucket}" >> .env

            touch /usr/local/bin/configure-consumer.sh
            echo "#!/bin/bash" >/usr/local/bin/configure-consumer.sh
            echo "/market_analysis/back/consumer/env/bin/python3 /market_analysis/back/consumer/consumer.py" >> /usr/local/bin/configure-consumer.sh
            chmod +x /usr/local/bin/configure-consumer.sh 
            
            echo '[Unit]' > /etc/systemd/system/consumer.service
            echo 'Description=Configure and launch Kafka consumer' >> /etc/systemd/system/consumer.service
            echo 'After=network.target' >> /etc/systemd/system/consumer.service
            echo ''  >> /etc/systemd/system/consumer.service
            echo '[Service]' >> /etc/systemd/system/consumer.service
            echo 'Type=oneshot' >> /etc/systemd/system/consumer.service
            echo 'ExecStart=/usr/local/bin/configure-consumer.sh' >> /etc/systemd/system/consumer.service
            echo 'RemainAfterExit=yes' >> /etc/systemd/system/consumer.service
            echo '' >> /etc/systemd/system/consumer.service
            echo '[Install]' >> /etc/systemd/system/consumer.service
            echo 'WantedBy=multi-user.target' >> /etc/systemd/system/consumer.service
          
            systemctl enable consumer.service
            systemctl start consumer.service
            # move dag
            cp /market_analysis/back/dag/main.py /root/airflow/dags/
          - EthAnalysisDataBucket:
              Fn::ImportValue: eth-analysis-data-bucket



  # Lambda Execution Role
  LambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: LambdaAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !Join
                      - ""
                      - - !ImportValue eth-analysis-data-bucket-arn
                        - "/*"
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:Query
                Resource:
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/BlockConfig
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/GasCostPerHour
              - Effect: Allow
                Action:
                  - ec2:DescribeInstances
                Resource:
                  "*"
              

  ProducerFunction:
    Type: AWS::Serverless::Function
    Properties:
      Handler: producer.handler
      Runtime: python3.10
      CodeUri: producer/
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 30
      Events:
        ProducerTimer:
          Type: Schedule
          Properties:
            Schedule: rate(2 minutes)
      Environment:
        Variables:
          BLOCK_CONFIG_TABLE: BlockConfig
          GAS_COST_TABLE: GasCostPerHour
          KAFKA_SERVER_AWS: True
          ETHERSCAN_API_KEY: !Ref ETHERSCANAPIKEY
          DYNAMODB_SERVER_AWS: True

  # EC2 Instance Role
  EC2InstanceProfile:
    Type: 'AWS::IAM::InstanceProfile'
    Properties:
      Roles:
        - !Ref EC2InstanceRole

  EC2InstanceRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: EC2Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:ListBucket
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:Query
                Resource:
                  - Fn::Sub:
                    - "${ImportedArn}/*"
                    - "${ImportedArn}"
                    - ImportedArn: !ImportValue eth-analysis-data-bucket-arn
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/BlockConfig
                  - !Sub arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/GasCostPerHour
              - Effect: Allow
                Action:
                  - ec2:DescribeInstances
                Resource:
                  "*"
Outputs:
  ProducerFunctionArn:
    Description: Producer Lambda Function ARN
    Value: !GetAtt ProducerFunction.Arn

